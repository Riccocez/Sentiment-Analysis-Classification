{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rcortez/anaconda/envs/python3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spell \n",
    "\n",
    "from nltk.sentiment import vader\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from Visualization import Visualization\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import cm as cmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('rebtel_w_lexicon.csv')\n",
    "data = data.dropna(subset=['Rel_title'], how='any')\n",
    "data = data.reset_index()\n",
    "del data['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = vader.SentimentIntensityAnalyzer()\n",
    "visual = Visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isrelevant(word):\n",
    "    \n",
    "    \n",
    "    neu_relevant = {'big','app','skype','call','calls',\n",
    "                    'scamming','billed',' messaging',\n",
    "                    'website','cant','logo','max','go',\n",
    "                    'get','cheap'}\n",
    "    \n",
    "    \n",
    "    pol =  v.polarity_scores(word)\n",
    "    \n",
    "    if pol['neg'] == 1 or pol['pos'] == 1:\n",
    "        return True\n",
    "    else:\n",
    "        if not word.isalpha():\n",
    "            if word == \"n't\":\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        else:\n",
    "            if word.lower() in neu_relevant:\n",
    "                return True\n",
    "            else:\n",
    "                \n",
    "                return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "def spell_checked(data,columnName):\n",
    "    \n",
    "    avoid = {\"``\",\",\",'\"',\"`\",\"-\"}\n",
    "    \n",
    "    crtd_messages = []\n",
    "\n",
    "    for m in data[columnName]:\n",
    "        \n",
    "        n_w = []\n",
    "        for word in nltk.word_tokenize(m):\n",
    "            if not isrelevant(word):\n",
    "                tmp_w = spell.correct(word)\n",
    "            else:\n",
    "                tmp_w = word\n",
    "            if tmp_w not in avoid:\n",
    "                n_w.append(tmp_w)\n",
    "        n_m = \" \".join(n_w) \n",
    "        crtd_messages.append(n_m)\n",
    "    \n",
    "    return crtd_messages   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['R_chk_title'] = spell_checked(data,'Rel_wp_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['R_chk_review'] = spell_checked(data,'Rel_wp_review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "wordnet = WordNetLemmatizer() \n",
    "\n",
    "def lemmatization(data,columnName):\n",
    "    lemmas = []\n",
    "    for message in data[columnName]:\n",
    "        tkn = nltk.word_tokenize(message)\n",
    "        wordnet_lemmas = [wordnet.lemmatize(token) for token in tkn] \n",
    "        lemmas.append(\" \".join(wordnet_lemmas))\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"Lems_title\"] = lemmatization(data,\"R_chk_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"Lems_review\"] = lemmatization(data,\"R_chk_review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stemming\n",
    "def stemming(data,columnName,stem): \n",
    "    lemmas = []\n",
    "    \n",
    "    snowball = SnowballStemmer(\"english\")\n",
    "    porter = PorterStemmer()\n",
    "    lancaster = LancasterStemmer()\n",
    "    \n",
    "    for message in data[columnName]:\n",
    "        tkn = nltk.word_tokenize(message)\n",
    "        if stem is \"snow\":\n",
    "            stemmed = [snowball.stem(token) for token in tkn]\n",
    "        elif stem is \"porter\":\n",
    "            stemmed = [porter.stem(token) for token in tkn]\n",
    "        elif stem is \"lanc\":\n",
    "            stemmed = [lancaster.stem(token) for token in tkn]\n",
    "        lemmas.append(\" \".join(stemmed))\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"Snow_title\"] = stemming(data,\"R_chk_title\",\"snow\")  \n",
    "data[\"Snow_review\"] = stemming(data,\"R_chk_review\",\"snow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[\"Porter_title\"] = stemming(data,\"R_chk_title\",\"porter\") \n",
    "data[\"Porter_review\"] = stemming(data,\"R_chk_review\",\"porter\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"Lanc_title\"] = stemming(data,\"R_chk_title\",\"lanc\") \n",
    "data[\"Lanc_review\"] = stemming(data,\"R_chk_review\",\"lanc\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Join text from title and review\n",
    "merged_text = []\n",
    "for i in range(len(data)):\n",
    "    mrgd = data[\"Lems_title\"][i] + data[\"Lems_review\"][i]\n",
    "    merged_text.append(mrgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mg_title_rev(data,approach):\n",
    "    # Join text from title and review\n",
    "    merged_text = []\n",
    "    for i in range(len(data)):\n",
    "        mrgd = data[approach + \"_title\"][i] + \" \" + data[approach + \"_review\"][i]\n",
    "        merged_text.append(mrgd)\n",
    "    return merged_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Lems_title_review'] = mg_title_rev(data,\"Lems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['Snow_title_review'] = mg_title_rev(data,\"Snow\")\n",
    "data['Porter_title_review'] = mg_title_rev(data,\"Porter\")\n",
    "data['Lanc_title_review'] = mg_title_rev(data,\"Lanc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data['title_review'] = merged_text\n",
    "data = data.reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a balanced train/test set\n",
    "r1_df = data.loc[data.Rating == 1]\n",
    "r2_df = data.loc[data.Rating == 2]\n",
    "r3_df = data.loc[data.Rating == 3]\n",
    "r4_df = data.loc[data.Rating == 4]\n",
    "r5_df = data.loc[data.Rating == 5]\n",
    "dfs = [r1_df, r2_df, r3_df, r4_df, r5_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_size = 0.7\n",
    "\n",
    "train_set = []\n",
    "test_set = []\n",
    "\n",
    "for df in dfs:\n",
    "    \n",
    "    train_size = int(len(df)* t_size)\n",
    "    \n",
    "    train_data = df[0:train_size].reset_index(drop=True)\n",
    "    test_data = df[train_size:].reset_index(drop=True)\n",
    "    \n",
    "    train_set.append(train_data)\n",
    "    test_set.append(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.concat(train_set[:],axis=0)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "\n",
    "df_test = pd.concat(test_set[:],axis=0)\n",
    "df_test = df_test.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df_train,df_test],axis=0)\n",
    "df = shuffle(df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Select method to train model (Lems/Snow/Porter/Lanc)\n",
    "method =  \"Lems\" \n",
    "x_train, y_train = df_train[method + '_title_review'].values, df_train['Rating'].values\n",
    "x_test, y_test = df_test[method + '_title_review'].values, df_test['Rating'].values\n",
    "\n",
    "x_train, y_train = x_train.reshape(x_train.shape[0],), y_train.reshape(y_train.shape[0],)\n",
    "x_test, y_test = x_test.reshape(x_test.shape[0],), y_test.reshape(y_test.shape[0],)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Extract text from title/reviews to build bag of words\n",
    "x_ti_data = data[\"Lems_title\"].values\n",
    "x_ti_data = x_ti_data.reshape(x_ti_data.shape[0],)\n",
    "\n",
    "x_re_data = data[\"Lems_review\"].values\n",
    "x_re_data = x_re_data.reshape(x_re_data.shape[0],)\n",
    "\n",
    "x_data = data['title_review'].values\n",
    "x_data = x_data.reshape(x_data.shape[0],)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract true labels from data\n",
    "y_data = df[\"Rating\"].values\n",
    "y_data = y_data.reshape(y_data.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# \n",
    "print(\"Classes: \")\n",
    "print(np.unique(y_train))\n",
    "\n",
    "# Summarize number of words\n",
    "print(\"Number of words: \")\n",
    "print(len(np.unique(np.hstack(x_train))))\n",
    "\n",
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in x_train]\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "# plot review length\n",
    "plt.boxplot(result)\n",
    "plt.title('Lenght of words in message reviews')\n",
    "plt.show()\n",
    "\n",
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in x_train]\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "# plot review length\n",
    "plt.boxplot(result)\n",
    "plt.title('Lenght of words in title reviews')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check distribution of classes\n",
    "#df.Rating.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up Vectorizer with the X most frequent words\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None,   \\\n",
    "                             max_features = 245)\n",
    "RANDOM_STATE = 0\n",
    "seed = 18\n",
    "np.random.seed(seed)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "n_features = 245\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ctyNames = df.Country.unique() \n",
    "ctyNames = np.append(ctyNames,['Unkown'])\n",
    "countries = {ctyNames[i]: i for i in range(0, len(ctyNames))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_country(ctyName,countries):\n",
    "    \n",
    "    if ctyName in countries:\n",
    "        value = countries[ctyName]\n",
    "    else:\n",
    "        value = countries['Unkown']\n",
    "\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Key_country'] = [map_country(cty,countries) for cty in df.Country]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ADD MORE FEATURES\n",
    "\"\"\"add_feats = df[['Reads','r_compound',\n",
    "                'r_negative', 'r_positive', 'r_neutral','t_compound',\n",
    "               't_negative', 't_positive', 't_neutral']]\"\"\"\n",
    "\n",
    "add_feats = df[['Reads','r_compound','r_negative', 'r_positive',\n",
    "                'r_neutral','t_compound','t_negative', 't_positive',\n",
    "                't_neutral','dayofweek','weekofyear','Key_country',\n",
    "                \"Useful\"]]\n",
    "a_feats = add_feats.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select method to train model (Lems/Snow/Porter/Lanc)\n",
    "method =  \"Porter\"\n",
    "add_feats = True\n",
    "\n",
    "x_fts = vectorizer.fit_transform(df[method + '_review'])#df[method + '_title_review'])\n",
    "x_fts = x_fts.toarray()\n",
    "\n",
    "\n",
    "if add_feats:\n",
    "    x_fts = np.concatenate((x_fts,a_feats),axis=1)\n",
    "\n",
    "\n",
    "train_size = 0.7\n",
    "train_size = int(len(x_fts)* train_size)\n",
    "\n",
    "x_train, y_train = x_fts[0:train_size], df['Rating'][0:train_size]\n",
    "x_test, y_test = x_fts[train_size:], df['Rating'][train_size:] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'label_probdist' and 'feature_probdist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-be11cc6b3265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##NAIVE BAYES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mNBclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'label_probdist' and 'feature_probdist'"
     ]
    }
   ],
   "source": [
    "##NAIVE BAYES\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ros = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "x_train, y_train = ros.fit_sample(x_train, y_train)\n",
    "x_test, y_test = ros.fit_sample(x_test, y_test)\n",
    "\n",
    "x_data  = np.concatenate((x_train,x_test),axis=0)\n",
    "y_data = np.concatenate((y_train,y_test),axis=0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add Bagging \n",
    "from random import randrange\n",
    "\n",
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(data_x,data_y, ratio=1.0):\n",
    "    \n",
    "    sample_x = []\n",
    "    sample_y = []\n",
    "    indexes = set()\n",
    "    n_sample = round(len(data_x) * ratio)\n",
    "    \n",
    "    while len(sample_x) < n_sample:\n",
    "        index = randrange(len(data_x))\n",
    "        sample_x.append(data_x[index])\n",
    "        sample_y.append(data_y[index])\n",
    "        indexes.add(index)\n",
    "        \n",
    "    test_index = [index for index in range(len(data_x)) if index not in indexes]    \n",
    "    test_x = [data_x[index] for index in test_index]\n",
    "    test_y = [data_y[index] for index in test_index]\n",
    "    \n",
    "    return (sample_x,sample_y), (test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rcortez/anaconda/envs/python3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/rcortez/anaconda/envs/python3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/rcortez/anaconda/envs/python3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/rcortez/anaconda/envs/python3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/rcortez/anaconda/envs/python3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/rcortez/anaconda/envs/python3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/rcortez/anaconda/envs/python3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "n_models = [150,170,190,200]\n",
    "#n_trees = 37\n",
    "ratio = 0.9\n",
    "clf_models = []\n",
    "model_predictions = []\n",
    "avgs = []\n",
    "\n",
    "for nmodel in range(100,110):\n",
    "    \n",
    "    #data,test = subsample(x_train,y_train,ratio)\n",
    "    clf = RandomForestClassifier(n_estimators= nmodel, n_jobs=-1,random_state=RANDOM_STATE)\n",
    "    #clf = svm.SVC(decision_function_shape='ovo',random_state=RANDOM_STATE,probability=True)\n",
    "    #clf = KNeighborsClassifier(n_neighbors=4)\n",
    "    clf.fit(x_train, y_train)\n",
    "    clf_models.append(clf)\n",
    "    \n",
    "    test_pred = clf.predict(x_test)\n",
    "    predictions = clf.score(x_test, y_test)\n",
    "    \n",
    "    rep = classification_report_imbalanced(test_pred, y_test)\n",
    "    \n",
    "    txt_rep = nltk.word_tokenize(rep[500:1000])\n",
    "    avg_total = [float(txt_rep[i]) for i in range(3,len(txt_rep))]\n",
    "    \n",
    "    avgs.append(avg_total)\n",
    "    model_predictions.append(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.57090239410681398,\n",
       " 0.574585635359116,\n",
       " 0.57274401473296499,\n",
       " 0.57090239410681398,\n",
       " 0.57274401473296499,\n",
       " 0.56906077348066297,\n",
       " 0.57090239410681398,\n",
       " 0.56906077348066297,\n",
       " 0.57090239410681398,\n",
       " 0.57090239410681398]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_model = clf_models[avgs.index(max(avgs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_preds = my_model.predict(x_test)\n",
    "print(classification_report_imbalanced(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rcortez/anaconda/envs/python3/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/rcortez/anaconda/envs/python3/lib/python3.5/site-packages/imblearn/metrics/classification.py:240: UndefinedMetricWarning: Sensitivity is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#cross_validation = StratifiedShuffleSplit(y_data, n_iter=15, test_size=0.3, random_state=RANDOM_STATE)\n",
    "cross_validation =   StratifiedKFold (y_test,n_folds=5, random_state=RANDOM_STATE)\n",
    "\n",
    "scores = []\n",
    "confusion_matrices = []\n",
    "clf_models = []\n",
    "avgs = []\n",
    "models_perf = []\n",
    "\n",
    "for n_model in range(10,40):\n",
    "    \n",
    "    for train_index,test_index in cross_validation:\n",
    "\n",
    "        clf = None\n",
    "    \n",
    "        dx_train, dy_train = x_train[train_index], y_train[train_index]\n",
    "        dx_test, dy_test = x_train[test_index], y_train[test_index]\n",
    "    \n",
    "        # Saving the scores.\n",
    "        clf = RandomForestClassifier(n_estimators= n_model, n_jobs=-1,random_state=RANDOM_STATE)\n",
    "        #clf = svm.SVC(decision_function_shape='ovo',random_state=RANDOM_STATE,probability=True)\n",
    "        clf.fit(dx_train,dy_train)\n",
    "        test_score = clf.score(dx_test, dy_test)\n",
    "        test_preds = clf.predict(dx_test)\n",
    "        rep = classification_report_imbalanced(dy_test, test_preds)\n",
    "        txt_rep = nltk.word_tokenize(rep[500:1000])\n",
    "        avg_total = [float(txt_rep[i]) for i in range(3,len(txt_rep))]\n",
    "        avgs.append(avg_total)\n",
    "        scores.append(test_score)\n",
    "\n",
    "\n",
    "        # Saving the confusion matrices.\n",
    "        data_classes_pred = clf.predict(dx_test) \n",
    "        cm = confusion_matrix(dy_test, test_preds)\n",
    "        confusion_matrices.append(cm)\n",
    "    \n",
    "    precision = np.mean([avgs[i][0] for i in range(len(avgs))])\n",
    "    recall = np.mean([avgs[i][1] for i in range(len(avgs))])\n",
    "    f_score = np.mean([avgs[i][3] for i in range(len(avgs))])\n",
    "    \n",
    "    performance = [precision,recall,f_score]\n",
    "    \n",
    "    models_perf.append(performance)\n",
    "    #print('Average precision :', str(np.mean([avgs[i][0] for i in range(len(avgs))])))\n",
    "    #print('Average recall :', str(np.mean([avgs[i][1] for i in range(len(avgs))])))\n",
    "    #print('Average F-score :', str(np.mean([avgs[i][3] for i in range(len(avgs))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.30599999999999999, 0.47400000000000003, 0.32599999999999996],\n",
       " [0.30600000000000005, 0.47400000000000003, 0.32599999999999996],\n",
       " [0.30600000000000005, 0.47400000000000003, 0.32599999999999996],\n",
       " [0.30599999999999999, 0.47399999999999992, 0.32600000000000001],\n",
       " [0.30599999999999999, 0.47399999999999998, 0.32600000000000001],\n",
       " [0.30599999999999999, 0.47399999999999998, 0.32600000000000001],\n",
       " [0.30599999999999999, 0.47399999999999998, 0.32600000000000007],\n",
       " [0.30599999999999999, 0.47400000000000003, 0.32599999999999996],\n",
       " [0.30600000000000005, 0.47400000000000003, 0.32600000000000001],\n",
       " [0.30599999999999999, 0.47400000000000003, 0.32600000000000001],\n",
       " [0.30599999999999999, 0.47400000000000009, 0.32600000000000001],\n",
       " [0.30599999999999994, 0.47400000000000003, 0.32599999999999996],\n",
       " [0.30599999999999999, 0.47400000000000003, 0.32599999999999996],\n",
       " [0.30599999999999999, 0.47399999999999998, 0.32599999999999996],\n",
       " [0.30599999999999994, 0.47399999999999998, 0.32600000000000001],\n",
       " [0.30599999999999999, 0.47400000000000003, 0.32599999999999996],\n",
       " [0.30599999999999999, 0.47399999999999998, 0.32599999999999996],\n",
       " [0.30599999999999999, 0.47399999999999998, 0.32599999999999996],\n",
       " [0.30599999999999994, 0.47399999999999992, 0.32599999999999996],\n",
       " [0.30599999999999999, 0.47400000000000003, 0.32599999999999996],\n",
       " [0.30600000000000005, 0.47400000000000003, 0.32599999999999996],\n",
       " [0.30600000000000011, 0.47400000000000003, 0.32600000000000001],\n",
       " [0.30600000000000005, 0.47399999999999998, 0.32599999999999996],\n",
       " [0.30599999999999999, 0.47399999999999998, 0.32599999999999996],\n",
       " [0.30600000000000005, 0.47399999999999992, 0.32600000000000001],\n",
       " [0.30599999999999999, 0.47400000000000003, 0.32599999999999996],\n",
       " [0.30599999999999999, 0.47400000000000003, 0.32600000000000001],\n",
       " [0.30599999999999999, 0.47399999999999998, 0.32600000000000001],\n",
       " [0.30599999999999994, 0.47400000000000003, 0.32599999999999996],\n",
       " [0.30599999999999994, 0.47399999999999998, 0.32599999999999996]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tip_labels = ['1' , '1', '2', '3', '4', '5']\n",
    "\n",
    "\n",
    "first = True\n",
    "cm = None\n",
    "\n",
    "for cm_iter in confusion_matrices:\n",
    "    if first:\n",
    "        cm = cm_iter.copy()\n",
    "        first = False\n",
    "    else:\n",
    "        \n",
    "        cm = cm + cm_iter\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(15,6))\n",
    "#cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "colorbar = axes.matshow(cm, cmap=cmap.gist_heat_r)\n",
    "fig.colorbar(colorbar)\n",
    "\n",
    "for (i, j), z in np.ndenumerate(cm):\n",
    "            axes.text(j, i, '{:0.2f}'.format(z), ha='center', va='center',color='gray')\n",
    "\n",
    "\n",
    "axes.set_xlabel('Predicted class', fontsize=15)\n",
    "axes.set_ylabel('True class', fontsize=15)\n",
    "\n",
    "axes.set_xticklabels(tip_labels)\n",
    "axes.set_yticklabels(tip_labels)\n",
    "\n",
    "axes.tick_params(labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#visual.counter(y_train,'y_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#forest = RandomForestClassifier(n_estimators = 37) \n",
    "clf = clf_models[3]\n",
    "#forest = forest.fit(x_train, y_train)\n",
    "#test_preds = clf.predict(x_test)\n",
    "\n",
    "#clf = svm.SVC(decision_function_shape='ovo',random_state=RANDOM_STATE)\n",
    "#clf = clf.fit(x_train, y_train)\n",
    "#test_preds = clf.predict(x_test)\n",
    "\n",
    "import pylab\n",
    "\n",
    "features = clf.feature_importances_\n",
    "\n",
    "\n",
    "pylab.figure(1,figsize=(20,6))\n",
    "x = range(len(features))\n",
    "pylab.xticks(x,rotation='vertical')\n",
    "\n",
    "pylab.yticks(fontsize=16)\n",
    "pylab.bar(x,features,align=\"center\",width=0.9)\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#visual.plot_confusion_matrix(y_test, test_preds, 'Prediction of Ratings with a RFC Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(classification_report_imbalanced(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf, 'best_RFC_text_SSS.pkl', compress=9)\n",
    "#model_clone = joblib.load('best_SVM_text.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
